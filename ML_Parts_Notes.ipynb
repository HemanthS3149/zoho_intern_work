{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO44qfX0hhFcMyHCCFjFlzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemanthS3149/zoho_intern_work/blob/Week-4/ML_Parts_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector,Matrix,Tensor**\n",
        "\n",
        "Vector: 1d array of numbers used to rep data points or feature sets in ML\n",
        "\n",
        "Matrix: A two-dimensional array of numbers. Matrices are used to represent datasets, where each row is a data point and each column is a feature.\n",
        "\n",
        "Tensor:An nd array of numbers.Tensors generalize vectors and matrices to higher dimensions and are used in dl"
      ],
      "metadata": {
        "id": "YEUxASRg7c4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "vector=np.array([1,2,3])\n",
        "print(vector,\"\\n\")\n",
        "\n",
        "matrix=np.array([[1,2,3,],[4,5,6]])\n",
        "print(matrix,\"\\n\")\n",
        "\n",
        "tensor=np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV40el-37ffC",
        "outputId": "e734571a-a3fa-40b0-9811-849afd5c23cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3] \n",
            "\n",
            "[[1 2 3]\n",
            " [4 5 6]] \n",
            "\n",
            "[[[1 2]\n",
            "  [3 4]]\n",
            "\n",
            " [[5 6]\n",
            "  [7 8]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent**: is an optimization algo used to minimize loss function by iteratively moving towards minimum val of the function. It updates weights and biases by calc the gradient of loss function with resp to other parameters and adjusting them in the oposite direction of the gradient."
      ],
      "metadata": {
        "id": "HYPz_UIS8eiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numpy, Torch, TorchVision**\n",
        "\n",
        "Numpy: a funda library for scientific computing in Python.Provides support for large multi-dimensional arrays and matrices along with colletion of math functions to operate on these arrays"
      ],
      "metadata": {
        "id": "cGOD0xlt9Rfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "array=np.array([1,2,3])\n",
        "print(array+1)\n",
        "print(array*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3y9ysQI8dtd",
        "outputId": "2a765689-87bc-4f79-88d2-d4a942c927b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4]\n",
            "[10 20 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Torch**:Core library of PyTorch, an open-source ML library for python. It provides tensor computation with strong GPU acceleration and dl capabilities."
      ],
      "metadata": {
        "id": "N8LZvP6q9qpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "tensor=torch.tensor([1,2,3])\n",
        "print(tensor+1)\n",
        "print(tensor*10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WbOczBB9lIW",
        "outputId": "7b2fdaef-7d82-48e0-f72f-e36b470ee979"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 3, 4])\n",
            "tensor([10, 20, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TorchVision:**A library that provides tools for CV,including popular datasets,model architectures and image transformations"
      ],
      "metadata": {
        "id": "b3_sATQ499-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets,transforms\n",
        "transform=transforms.Compose([transforms.ToTensor()])#transforms can perform ops on image data like converting them to tensors in this case\n",
        "mnist_dataset=datasets.MNIST(root='mnist_data',train=True,transform=transform,download=True)\n",
        "#transform=transform applies the specified transformations to the images\n",
        "#transforms.Compose allows you to chain multiple transformations together"
      ],
      "metadata": {
        "id": "0Upo7N1Q97XI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1921b5a9-1975-47e4-eb67-5bd91a3f912b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5066778.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134354.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1246475.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8463140.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization:**process of converting algorithms from operating on single values (scalar operations) to operating on vectors or matrices (vector operations). This process can significantly speed up computation .\n"
      ],
      "metadata": {
        "id": "umrbbCwO_aDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Model\n",
        "A linear regression model predicts the output variable\n",
        "y as a linear combination of input variables\n",
        "X, i.e.,\n",
        "y=Xw+b, where\n",
        "w is the weight vector and\n",
        "b is the bias."
      ],
      "metadata": {
        "id": "ICrXO1Rq_lkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset,Weight,Bias:**\n",
        "Dataset: A collection of data points used for training and evaluating models.\n",
        "\n",
        "Weight: Parameters that the model learns during training, determining the\n",
        "influence of input features on the output.\n",
        "\n",
        "Bias: An additional parameter that allows the model to fit the data better by providing an offset."
      ],
      "metadata": {
        "id": "eiqvrzqk_ys5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters: Values learned by the model during training (e.g., weights and biases).\n",
        "\n",
        "Hyperparameters: Values set before the training process that control the learning process (e.g., learning rate, number of epochs)."
      ],
      "metadata": {
        "id": "VHUpIX6JCUa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Prediction Using Linear Regression Model**\n"
      ],
      "metadata": {
        "id": "Wj23m0LCCoRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LinearRegressionModel,self).__init__()\n",
        "    self.linear=nn.Linear(1,1) #1 input feature, 1 output\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear(x)#when the model is called, it applies the linear transformation to the input 'x'.\n",
        "\n",
        "#Generating some data\n",
        "x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
        "\n",
        "model=LinearRegressionModel()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "#Training the model\n",
        "for epoch in range(100):\n",
        "  model.train() #sets the model to training mode\n",
        "  optimizer.zero_grad()\n",
        "  outputs=model(x_train) #performs the forward pass to compute the model's prediction for 'x_train'\n",
        "  loss=criterion(outputs,y_train)\n",
        "  loss.backward() #backpropagates the loss,computing the gradient of the loss wrt model params\n",
        "  optimizer.step()#Updates the model parameters using the computed gradients\n",
        "  if(epoch+1)%10==0:\n",
        "    print(f'Epoch [{epoch+1}/100], Loss:{loss.item():.4f}')\n",
        "\n",
        "#Generating predictions\n",
        "model.eval() #sets the model to evaluation mode, disabling\n",
        "predicted=model(x_train).detach().numpy() #detach is used to detach the output from the computational graph\n",
        "print(predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKhBMfgc-w5L",
        "outputId": "20c9209b-916d-450a-fcbb-7fa8c90df6b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss:2.1024\n",
            "Epoch [20/100], Loss:0.0661\n",
            "Epoch [30/100], Loss:0.0127\n",
            "Epoch [40/100], Loss:0.0107\n",
            "Epoch [50/100], Loss:0.0100\n",
            "Epoch [60/100], Loss:0.0095\n",
            "Epoch [70/100], Loss:0.0089\n",
            "Epoch [80/100], Loss:0.0084\n",
            "Epoch [90/100], Loss:0.0079\n",
            "Epoch [100/100], Loss:0.0074\n",
            "[[2.1388113]\n",
            " [4.0672636]\n",
            " [5.9957156]\n",
            " [7.924168 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function Vs Cost Function**\n",
        "\n",
        "Loss Function: Measures the difference between the predicted and actual values for a single data point.\n",
        "\n",
        "Cost Function: Measures the average difference over the entire dataset."
      ],
      "metadata": {
        "id": "22D-R2IkEvUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Squared Error:**Is a common loss function for regression problems, calc as the average of the squared differences between the predicted and actual values.\n"
      ],
      "metadata": {
        "id": "0eNTk9wkFeDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y_pred,y_true):\n",
        "  return ((y_pred-y_true)**2).mean()"
      ],
      "metadata": {
        "id": "GIrzOTjCEifq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward vs. Backward Function\n",
        "\n",
        "Forward Function: Computes the output of the model from the input.\n",
        "\n",
        "Backward Function: Computes the gradient of the loss function with respect to the model parameters (backpropagation).\n",
        "\n",
        "Gradient descent updates the weights and biases by moving them in the direction that reduces the loss function\n",
        "\n",
        "**Epoch:**One complete pass through the entire training dataset\n",
        "\n",
        "Increasing the no of epochs allows the model to learn better from the data, reducing the loss.\n",
        "\n",
        "Learning Rate: hyperparameter that controls how much to change the model parameters in response to the estimated error\n",
        "\n",
        "Learning rate: hyperparameter that controls how much to change the model params in response to estimated error each time the model weights are updated\n"
      ],
      "metadata": {
        "id": "ANzL1zCCF0Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer in Pytorch**:Adjusts the weights and biases of the model to minimize the loss function(SGD,Adam,RMSprop)"
      ],
      "metadata": {
        "id": "bTavKQmuRuzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "xI-X2z2MFxZO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Part B**\n"
      ],
      "metadata": {
        "id": "bDCi-RamaHlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression\n",
        "Purpose: Used for predicting continuous values.\n",
        "Model:\n",
        "y=Xw+b, where\n",
        "y is the output,\n",
        "X is the input features, w is the weights, and b is the bias.\n",
        "\n",
        "Loss Function: Mean Squared Error (MSE).\n",
        "\n",
        "Output: Continuous value.\n",
        "\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "Purpose: Used for binary classification problems.\n",
        "\n",
        "Model: y=σ(Xw+b),\n",
        "where\n",
        "σ is the sigmoid function σ(z)= 1+e −z\n",
        "\n",
        "1.\n",
        "Loss Function: Binary Cross-Entropy.\n",
        "\n",
        "Output: Probability (between 0 and 1)."
      ],
      "metadata": {
        "id": "gwhvIBoLbKC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Batch size while training**:Refers to the number of training examples utilized in 1 iteration.\n",
        "\n",
        "It affects the training speed,memory usage and gradient estimation(smaller batch sizes produce noisier gradient estimates which can help escape local minima)\n",
        "\n"
      ],
      "metadata": {
        "id": "PwvjVWVOmnBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Entropy:**Used in classification problems, measures the performance of a classification model whose output is prob bw 0 and 1. (Cross entropy loss is in -ve)"
      ],
      "metadata": {
        "id": "DHhoNpxwm971"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy vs Loss Function:**\n",
        "\n",
        "Accuracy: Measures the no of correct predictions. Metric to evaluate the model's performance.\n",
        "\n",
        "Loss Function: Measures how well model is performing during training. Used to update the model's weights."
      ],
      "metadata": {
        "id": "eRhbF74mnZ0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Hidden Layer:Deep NN**\n",
        "\n",
        "Hidden layer:Intermediate layers bw ip and op layers in a nn.They allow the network to learn complex patterns and reps\n",
        "\n",
        "Deep NN:A network with multiple hidden layers. These networks can model complex relationships and achieve better performance on tasks like image recognition and NLP"
      ],
      "metadata": {
        "id": "awoiJIosn4AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train a Model with 1 Hidden Layer**"
      ],
      "metadata": {
        "id": "IgTtjyvGoVPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "#Generating synthetic data\n",
        "np.random.seed(42)#seed for NumPy's random generator to ensure reproducibility\n",
        "x_train=np.random.rand(100,1).astype(np.float32)#generates 100 random numbers b/w 0 and 1,reshaped into 100x1 array\n",
        "y_train=3.5*x_train+np.random.randn(100,1).astype(np.float32)*0.5 #y=3.5x+noise term\n",
        "\n",
        "x_train=torch.from_numpy(x_train)\n",
        "y_train=torch.from_numpy(y_train)\n",
        "\n",
        "#Defining the model with one hidden layer\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleNN,self).__init__()\n",
        "    self.hidden=nn.Linear(1,10)#Hidden layer with 10 neurons and 1 input feature\n",
        "    self.relu=nn.ReLU()\n",
        "    self.output=nn.Linear(10,1) #Output layer with 10 ip features and 1 output feature\n",
        "\n",
        "  def forward(self,x): #passes input x into hidden layer, applies ReLU activation and passes result through output layer\n",
        "    x=self.hidden(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.output(x)\n",
        "    return x\n",
        "\n",
        "model=SimpleNN()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "#Training the model\n",
        "num_epochs=100\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad() #clears the gradients of all optimized parameters\n",
        "  outputs=model(x_train)\n",
        "  loss=criterion(outputs,y_train)\n",
        "  loss.backward() #Backpropagates the loss, computing the gradient of the loss wrt other params\n",
        "  optimizer.step() #updates the model parameters using the computed gradients\n",
        "  if(epoch+1)%10==0:\n",
        "    print(f'Epoch[{epoch+1}/{num_epochs}],Loss:{loss.item():.4f}')"
      ],
      "metadata": {
        "id": "VsVTmjyISbfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8653e9f-2843-4813-8a7d-42f30062fa71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[10/100],Loss:1.5267\n",
            "Epoch[20/100],Loss:0.9884\n",
            "Epoch[30/100],Loss:0.8673\n",
            "Epoch[40/100],Loss:0.8144\n",
            "Epoch[50/100],Loss:0.7722\n",
            "Epoch[60/100],Loss:0.7327\n",
            "Epoch[70/100],Loss:0.6948\n",
            "Epoch[80/100],Loss:0.6585\n",
            "Epoch[90/100],Loss:0.6238\n",
            "Epoch[100/100],Loss:0.5905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of GPU in training:**\n",
        "\n",
        "Speed:GPUs can process multiple ops in parallel, significantly speeding up training.\n",
        "\n",
        "Efficiency: GPUs are optimized for matrix and vector ops, mostly used in DL\n"
      ],
      "metadata": {
        "id": "mjh0HozaoTMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Device Parameter:**Specifies whether to run the model on a CPU or GPU.(Its easy to move tensors and models bw devices)"
      ],
      "metadata": {
        "id": "o5TJfhxZvK4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing training times on CPU and GPU**\n"
      ],
      "metadata": {
        "id": "vJIpzTe1vvj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train_model(device):\n",
        "  model.to(device) #moves the model to the specific device\n",
        "  x_train_device=x_train.to(device)\n",
        "  y_train_device=y_train.to(device)#moves the training data to the specified device\n",
        "\n",
        "  #Initializing optimizer and loss func\n",
        "  optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "  criterion=nn.MSELoss()\n",
        "\n",
        "  #Training loop with time measurement\n",
        "  start_time=time.time()\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(x_train_device)\n",
        "    loss=criterion(outputs,y_train_device)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  end_time=time.time()\n",
        "  return end_time-start_time"
      ],
      "metadata": {
        "id": "jyhnYMBAvJ51"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training on CPU\n",
        "cpu_time=train_model(torch.device('cpu'))\n",
        "print(f'Training time on CPU:{cpu_time:.2f} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHzBHRD3wwTm",
        "outputId": "36ef861a-1add-4f43-b36f-45385b31592c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time on CPU:0.08 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():#Checks if GPU is available\n",
        "    gpu_time=train_model(torch.device('cuda'))#CUDA(cpmplete unified device architecture) is an API by NVIDIA\n",
        "    print(f\"Training time on GPU:{gpu_time:.2f} seconds\")\n",
        "else:\n",
        "  print(\"GPU is not available\")\n",
        "#For smaller models or datasets, the overhead of transferring data bw CPU and GPU can result in slower perf of GPU\n",
        "#GPU Warm up time could also be a reason"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxnCy9gGxffZ",
        "outputId": "6126eee6-af4f-47fe-c937-031c105b84e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time on GPU:0.09 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Part C**'"
      ],
      "metadata": {
        "id": "5PCDeXaQ2BzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Network**:is a DL algo used for analyzing visual data.They are designed to automatically and adaptively learn spatial hierarchies of features through back prop by using multiple building blocks such as convolution layers,pooling layers and fully connected layers"
      ],
      "metadata": {
        "id": "I3Tx33fR733v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Components:**\n",
        "\n",
        "1. Convolution layer(nn.Conv2d)\n",
        "2.Activation layer(nn.ReLU)\n",
        "3.Pooling Layer(nn.MaxPool2d)\n",
        "4.Sequential Model(nn.Sequential)"
      ],
      "metadata": {
        "id": "7xIB63Hr8PPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strides:**defines the step size with which the filter moves across the input image\n",
        "\n",
        "eg) Input size:5x5, Filter size:3x3,Stride 2=> the filter skips every other pixel, generating a 2x2 output\n",
        "\n",
        "**Padding:**involves adding extra pixels around the border of the input image.This can be useful for maintaining the spatial dimensions of op after convolution\n",
        "\n",
        "eg) No padding=> reduces the output dimensions, a 5x5 input with 3x3 filter and no padding results in 3x3 output.\n",
        "\n",
        "Zero Padding: Maintains the output dimensions. A 5x5 input with a 3x3 filter and padding of 1 results in 5x5 output.\n",
        "\n",
        "Fully Connected Layers: in which each neuron is connected to every neuron in the previous layer. These are used at the end of a CNN to perform classification.\n",
        "\n",
        "eg)ip from a con layer: a 1d vector of size 128(learned from the previous layer)\n",
        "\n",
        "Fully Connected Layer: Connects this 128d input to a 10d output (for classification task with 10 classes)"
      ],
      "metadata": {
        "id": "fTe4FJr2Cg4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple CNN example\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#padding=1 adds 1 pixel of zero-padding on each side of the input\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN,self).__init__()#calls the constructor of the parent class nn.Module to initialize the base components\n",
        "    self.conv1=nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)#kernel_size=> size of each filter\n",
        "    self.pool=nn.MaxPool2d(kernel_size=2,stride=2,padding=0) #For downsampling the input along the spatial dimensions(width and height) but retains the most important features\n",
        "    self.conv2=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
        "    self.fc1=nn.Linear(64*7*7,128)#computed based on the output size of the last pooling layer,64*7*7=>no of input features,128=>no of output features\n",
        "    self.fc2=nn.Linear(128,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.pool(F.relu(self.conv1(x)))#applies 1st conv layer, followed by a ReLU activation function,then a maxpooling operation\n",
        "    x=self.pool(F.relu(self.conv2(x)))\n",
        "    x=x.view(-1,64*7*7)#flattens the output from conv layers to a vector of size 64*7*7 to feed into the fully connected layer. -1=>auto infer the batch size\n",
        "    x=F.relu(self.fc1(x))#Applies 1st FCL followed by a relu\n",
        "    x=self.fc2(x)#applies 2nd fcl to get the final output\n",
        "    return x\n",
        "model=SimpleCNN()\n",
        "print(model)#prints the model arch, showing the layers and their configs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-OM1ZzAxzns",
        "outputId": "a8d4949d-6572-44c8-a5c4-5c61f43291d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting:**occurs when a model performs well on the training data but poorly on unseen test data. It means the model has learned the training data too well, including its noise and outliers."
      ],
      "metadata": {
        "id": "R1QE_1YVJLDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Strategies to avoid overfitting:**\n",
        "\n",
        "1.Learning rate schedular=>to improve convergence and avoid overfitting\n",
        "\n",
        "2.Weight decay=>adding a regularization term to the loss function to penalize large weights\n",
        "\n",
        "3.Gradient clipping=> Clipping the gradients during back prop to prevent exploding gradient problem which may destabilize the training process"
      ],
      "metadata": {
        "id": "hGzp66iIJQkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=30,gamma=0.1)\n",
        "#optimizer=torch.optim.SGD(model.parameters(),lr=0.01,weight_decay=0.001)\n",
        "#torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=2.0)"
      ],
      "metadata": {
        "id": "q2uyt84i_KOI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regulaalrization of Data**\n",
        "\n",
        "**Data Normalization:**involves scaling the input data to a standard range (e.g., 0 to 1 or -1 to 1). It helps improve the convergence rate of the gradient descent algorithm.\n"
      ],
      "metadata": {
        "id": "tC7CNVQhKk7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))#specifies mean and sd\n",
        "])"
      ],
      "metadata": {
        "id": "Ii_d-L5jJuj8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation:**artificially increases the size of the training dataset by creating modified versions of images in the dataset. This helps improve the model's generalization."
      ],
      "metadata": {
        "id": "NLjRYQyGLD66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),#flip image horizontally with prob of 0.5\n",
        "    transforms.RandomRotation(10),#randomly rotates input image from -10 to 10 degrees\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "SsxpyJBnLC4o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Signal Processing Techniques: ** involve analyzing, modifying and synthesizing signals.(Fourier transform,filtering, time-domain analysis) for audio processing, communications and image processing."
      ],
      "metadata": {
        "id": "k8xsNp5ELs-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ONNX:** (Open neural network exchange) is an open source format for representing ML Models. Allows models trained in different frameworks to be transferred bw them, improving deployement flexibility."
      ],
      "metadata": {
        "id": "EVz_pNYjMDaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QbMLEEVLlT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}